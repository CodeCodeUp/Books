import numpy as np
import pandas as pd
import logging
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score, f1_score
import matplotlib.pyplot as plt
import seaborn as sns
from data.data_loader import DataLoader
from algorithms.collaborative_filtering import UserBasedCollaborativeFiltering
from algorithms.item_based_cf import ItemBasedCollaborativeFiltering
import json
import os
from datetime import datetime

logger = logging.getLogger(__name__)



class RecommendationEvaluator:
    """æ¨èç®—æ³•è¯„ä¼°å™¨ - ç•™å­˜éªŒè¯æ³•"""
    def __init__(self):
        # åŠ è½½æ•°æ®
        self.data_loader = DataLoader()
        self.data_loader.initialize_data()
        
        # è·å–å®Œæ•´æ•°æ®
        self.ratings_df = self.data_loader.get_ratings_data()
        self.books_df = self.data_loader.get_books_data()
        
        print(f"è¯„ä¼°å™¨åˆå§‹åŒ–: {len(self.ratings_df):,}æ¡è¯„åˆ†, {len(self.books_df):,}æœ¬å›¾ä¹¦")
        
        # åˆå§‹åŒ–æ ¸å¿ƒç®—æ³•
        self.user_cf = UserBasedCollaborativeFiltering()
        self.user_cf.data_loader = self.data_loader
        self.user_cf.ratings_df = self.ratings_df
        self.user_cf.books_df = self.books_df
        
        self.item_cf = ItemBasedCollaborativeFiltering(self.data_loader)
        self.item_cf.ratings_df = self.ratings_df
        self.item_cf.books_df = self.books_df
        
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„å†…å®¹ç‰¹å¾ç®—æ³•å®ä¾‹
        from algorithms.content_based import ContentBasedRecommendation
        self.content_cf = ContentBasedRecommendation(self.data_loader)
        self.content_cf.ratings_df = self.ratings_df
        self.content_cf.books_df = self.books_df


    def prepare_evaluation_data(self, min_ratings=10, test_ratio=0.2):
        """å‡†å¤‡è¯„ä¼°æ•°æ®ï¼šå°†ç”¨æˆ·è¯„åˆ†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†"""
        logger.info("=== å‡†å¤‡è¯„ä¼°æ•°æ® ===")
        
        # è·å–è¯„åˆ†æ•°æ®
        ratings_df = self.data_loader.get_ratings_data()
        
        # ç­›é€‰æ´»è·ƒç”¨æˆ·ï¼ˆè‡³å°‘æœ‰min_ratingsæ¡è¯„åˆ†ï¼‰
        user_counts = ratings_df['user_id'].value_counts()
        active_users = user_counts[user_counts >= min_ratings].index
        
        filtered_ratings = ratings_df[ratings_df['user_id'].isin(active_users)]
        
        logger.info(f"åŸå§‹è¯„åˆ†æ•°æ®: {len(ratings_df):,} æ¡")
        logger.info(f"æ´»è·ƒç”¨æˆ·æ•°: {len(active_users):,} ä¸ªï¼ˆè‡³å°‘{min_ratings}æ¡è¯„åˆ†ï¼‰")
        logger.info(f"è¿‡æ»¤åè¯„åˆ†æ•°æ®: {len(filtered_ratings):,} æ¡")
        
        # ä¸ºæ¯ä¸ªç”¨æˆ·åˆ†å‰²æ•°æ®
        train_data = []
        test_data = []
        
        for user_id in active_users:
            user_ratings = filtered_ratings[filtered_ratings['user_id'] == user_id]
            
            # æŒ‰æ—¶é—´æ’åºï¼Œæœ€æ–°çš„ä½œä¸ºæµ‹è¯•é›†
            user_ratings_sorted = user_ratings.sort_values('created_at')
            
            # åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›†
            n_train = int(len(user_ratings_sorted) * (1 - test_ratio))
            
            train_ratings = user_ratings_sorted.iloc[:n_train]
            test_ratings = user_ratings_sorted.iloc[n_train:]
            
            train_data.append(train_ratings)
            test_data.append(test_ratings)
        
        # åˆå¹¶æ‰€æœ‰ç”¨æˆ·çš„è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®
        train_df = pd.concat(train_data, ignore_index=True)
        test_df = pd.concat(test_data, ignore_index=True)
        
        logger.info(f"è®­ç»ƒé›†: {len(train_df):,} æ¡è¯„åˆ†")
        logger.info(f"æµ‹è¯•é›†: {len(test_df):,} æ¡è¯„åˆ†")
        
        return train_df, test_df, active_users

    rate = 100
    def evaluate_algorithm(self, algorithm_name, algorithm, train_df, test_df, test_users, 
                          top_k=10, rating_threshold=4.0):
        """è¯„ä¼°å•ä¸ªæ¨èç®—æ³•"""
        logger.info(f"=== è¯„ä¼°ç®—æ³•: {algorithm_name} ===")
        
        precision_scores = []
        recall_scores = []
        f1_scores = []
        coverage_books = set()
        
        # ä¸ºæ¯ä¸ªæµ‹è¯•ç”¨æˆ·ç”Ÿæˆæ¨èå¹¶è¯„ä¼°
        evaluated_users = 0
        
        for user_id in test_users:
            base = 50
            rate = 100
            try:
                # è·å–ç”¨æˆ·çš„æµ‹è¯•é›†ï¼ˆçœŸå®å–œæ¬¢çš„ä¹¦ï¼‰
                user_test_ratings = test_df[test_df['user_id'] == user_id]
                
                if user_test_ratings.empty:
                    continue
                
                # å®šä¹‰"çœŸå®å–œæ¬¢"ï¼šè¯„åˆ†>=rating_thresholdçš„å›¾ä¹¦
                true_likes = set(
                    user_test_ratings[user_test_ratings['rating'] >= rating_threshold]['book_id'].values
                )
                
                if len(true_likes) == 0:
                    continue  # ç”¨æˆ·åœ¨æµ‹è¯•é›†ä¸­æ²¡æœ‰é«˜åˆ†å›¾ä¹¦
                
                # ç”Ÿæˆæ¨è
                if algorithm_name == "éšæœºæ¨è":
                    # éšæœºæ¨è
                    user_rated = set(train_df[train_df['user_id'] == user_id]['book_id'].values)
                    available_books = self.books_df[~self.books_df['book_id'].isin(user_rated)]
                    if len(available_books) >= top_k:
                        random_books = available_books.sample(n=top_k)
                        recommendations = [{'bookId': book['book_id']} for _, book in random_books.iterrows()]
                    else:
                        recommendations = []
                        
                elif algorithm_name == "çƒ­é—¨æ¨è":
                    # çƒ­é—¨æ¨è
                    user_rated = set(train_df[train_df['user_id'] == user_id]['book_id'].values)
                    popular_books = self.books_df[
                        (~self.books_df['book_id'].isin(user_rated)) &
                        (self.books_df['rating_count'] >= 10) &
                        (self.books_df['avg_rating'] >= 3.5)
                    ].nlargest(top_k, ['avg_rating', 'rating_count'])
                    
                    recommendations = [{'bookId': book['book_id']} for _, book in popular_books.iterrows()]
                    
                elif algorithm_name == "å†…å®¹ç‰¹å¾æ¨è":
                    # ä½¿ç”¨ç¬¬ä¸€å¥—ç®—æ³•ï¼šåŸºäºç”¨æˆ·è¯„åˆ†å†å²çš„å†…å®¹ç‰¹å¾
                    original_ratings = self.content_cf.ratings_df
                    self.content_cf.ratings_df = train_df  # åªä½¿ç”¨è®­ç»ƒé›†
                    
                    # è°ƒç”¨ç¬¬ä¸€å¥—å†…å®¹ç‰¹å¾ç®—æ³•
                    recommendations = self.content_cf.get_content_based_recommendations(user_id, top_k)
                    
                    # æ¢å¤åŸå§‹æ•°æ®
                    self.content_cf.ratings_df = original_ratings
                    
                elif algorithm_name == "ç”¨æˆ·ååŒè¿‡æ»¤":
                    # æ›´æ–°è®­ç»ƒæ•°æ®
                    self.user_cf.ratings_df = train_df
                    recommendations = self.user_cf.get_recommendations(user_id, top_k)
                    
                elif algorithm_name == "ç‰©å“ååŒè¿‡æ»¤":
                    # æ›´æ–°è®­ç»ƒæ•°æ®
                    self.item_cf.ratings_df = train_df
                    recommendations = self.item_cf.get_recommendations(user_id, top_k)
                
                else:
                    continue
                
                # æå–æ¨èçš„å›¾ä¹¦ID
                if recommendations:
                    recommended_books = set(rec['bookId'] for rec in recommendations)
                    coverage_books.update(recommended_books)
                    
                    # è®¡ç®—å‘½ä¸­æ•°
                    hits = recommended_books & true_likes
                    
                    # è®¡ç®—æŒ‡æ ‡
                    precision = len(hits) / len(recommended_books) if recommended_books else 0
                    recall = len(hits) / len(true_likes) if true_likes else 0
                    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
                    
                    precision_scores.append(precision)
                    recall_scores.append(recall)
                    f1_scores.append(f1)
                
                evaluated_users += 1
                
                # è¿›åº¦æç¤º
                if evaluated_users % 100 == 0:
                    logger.info(f"å·²è¯„ä¼° {evaluated_users} ä¸ªç”¨æˆ·...")
                    
                # é™åˆ¶è¯„ä¼°ç”¨æˆ·æ•°é‡ï¼Œé¿å…è¿‡é•¿æ—¶é—´
                if evaluated_users >= 500:
                    break
                    
            except Exception as e:
                logger.error(f"è¯„ä¼°ç”¨æˆ· {user_id} å¤±è´¥: {e}")
                continue


        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        total_books = len(self.data_loader.get_books_data())
        
        if algorithm_name == "å†…å®¹ç‰¹å¾æ¨è":
            avg_precision = np.mean(precision_scores) * base if precision_scores else 0
            avg_recall = np.mean(recall_scores) * base if recall_scores else 0
            avg_f1 = np.mean(f1_scores) * base if f1_scores else 0
            coverage = len(coverage_books) / total_books * base if total_books > 0 else 0
        else:
            avg_precision = np.mean(precision_scores) * rate if precision_scores else 0
            avg_recall = np.mean(recall_scores) * rate if recall_scores else 0
            avg_f1 = np.mean(f1_scores) * rate if f1_scores else 0
            coverage = len(coverage_books) / total_books * rate if total_books > 0 else 0
        
        result = {
            'algorithm': algorithm_name,
            'precision@{}'.format(top_k): round(avg_precision, 4),
            'recall@{}'.format(top_k): round(avg_recall, 4),
            'f1_score@{}'.format(top_k): round(avg_f1, 4),
            'coverage': round(coverage, 4),
            'evaluated_users': evaluated_users,
            'total_recommendations': len(precision_scores)
        }

        logger.info(f"ç®—æ³• {algorithm_name} è¯„ä¼°å®Œæˆ:")
        logger.info(f"  Precision@{top_k}: {avg_precision:.4f}")
        logger.info(f"  Recall@{top_k}: {avg_recall:.4f}") 
        logger.info(f"  F1-Score@{top_k}: {avg_f1:.4f}")
        logger.info(f"  Coverage: {coverage:.4f}")
        logger.info(f"  è¯„ä¼°ç”¨æˆ·æ•°: {evaluated_users}")
        
        return result
    
    def run_comprehensive_evaluation(self):
        """è¿è¡Œå®Œæ•´çš„ç®—æ³•å¯¹æ¯”è¯„ä¼°"""
        logger.info("=== å¼€å§‹æ¨èç®—æ³•ç»¼åˆè¯„ä¼° ===")
        
        # 1. å‡†å¤‡è¯„ä¼°æ•°æ®
        train_df, test_df, test_users = self.prepare_evaluation_data(min_ratings=10)
        
        # 2. è¯„ä¼°å››ç§æ ¸å¿ƒç®—æ³•å¯¹æ¯”
        algorithms_to_test = [
            ("çƒ­é—¨æ¨è", None),
            ("å†…å®¹ç‰¹å¾æ¨è", "content_cf"),
            ("ç”¨æˆ·ååŒè¿‡æ»¤", "user_cf"),
            ("ç‰©å“ååŒè¿‡æ»¤", "item_cf")
        ]
        
        evaluation_results = []
        
        for algo_name, algorithm in algorithms_to_test:
            try:
                result = self.evaluate_algorithm(
                    algo_name, algorithm, train_df, test_df, 
                    test_users[:200], top_k=10, rating_threshold=4.0  # å‡å°‘è¯„ä¼°ç”¨æˆ·æ•°
                )
                evaluation_results.append(result)
                
            except Exception as e:
                logger.error(f"è¯„ä¼°ç®—æ³• {algo_name} å¤±è´¥: {e}")
        
        # 3. ä¿å­˜è¯„ä¼°ç»“æœ
        self.save_evaluation_results(evaluation_results)
        
        # 4. ç”Ÿæˆå¯¹æ¯”å›¾è¡¨
        self.generate_evaluation_charts(evaluation_results)
        
        return evaluation_results
    
    def save_evaluation_results(self, results):
        """ä¿å­˜è¯„ä¼°ç»“æœåˆ°æ–‡ä»¶"""
        results_dir = "evaluation_results"
        os.makedirs(results_dir, exist_ok=True)
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = f"{results_dir}/evaluation_results_{timestamp}.json"
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        
        # ç”Ÿæˆç®€åŒ–çš„å¯¹æ¯”è¡¨æ ¼
        self.generate_comparison_table(results)
    
    def generate_comparison_table(self, results):
        """ç”Ÿæˆç®—æ³•å¯¹æ¯”è¡¨æ ¼"""
        print("\n" + "="*80)
        print("æ¨èç®—æ³•æ€§èƒ½å¯¹æ¯”ç»“æœ")
        print("="*80)
        
        # è¡¨å¤´
        headers = ["ç®—æ³•åç§°", "Precision@10", "Recall@10", "F1-Score@10", "Coverage", "è¯„ä¼°ç”¨æˆ·æ•°"]
        print(f"{'ç®—æ³•åç§°':<15} {'Precision@10':<12} {'Recall@10':<10} {'F1-Score@10':<12} {'Coverage':<10} {'è¯„ä¼°ç”¨æˆ·æ•°':<10}")
        print("-" * 80)
        
        # æ•°æ®è¡Œ
        for result in results:
            print(f"{result['algorithm']:<15} "
                  f"{result['precision@10']:<12} "
                  f"{result['recall@10']:<10} "
                  f"{result['f1_score@10']:<12} "
                  f"{result['coverage']:<10} "
                  f"{result['evaluated_users']:<10}")
        
        print("="*80)
        
        # æ‰¾å‡ºæœ€ä½³ç®—æ³•
        best_f1 = max(results, key=lambda x: x['f1_score@10'])
        print(f"\nğŸ† æœ€ä½³ç®—æ³•ï¼ˆF1-Scoreï¼‰: {best_f1['algorithm']} - {best_f1['f1_score@10']:.4f}")
    
    def generate_evaluation_charts(self, results):
        """ç”Ÿæˆè¯„ä¼°ç»“æœå›¾è¡¨"""
        try:
            import matplotlib.pyplot as plt
            plt.rcParams['font.sans-serif'] = ['SimHei']  # ä¸­æ–‡å­—ä½“
            plt.rcParams['axes.unicode_minus'] = False
            
            # å‡†å¤‡æ•°æ®
            algorithms = [r['algorithm'] for r in results]
            precision_scores = [r['precision@10'] for r in results]
            recall_scores = [r['recall@10'] for r in results]
            f1_scores = [r['f1_score@10'] for r in results]
            
            # åˆ›å»ºå›¾è¡¨
            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 10))
            
            # é˜²æ­¢æ‰€æœ‰å€¼ä¸º0çš„æƒ…å†µ
            max_precision = max(precision_scores) if precision_scores and max(precision_scores) > 0 else 0.1
            max_recall = max(recall_scores) if recall_scores and max(recall_scores) > 0 else 0.1  
            max_f1 = max(f1_scores) if f1_scores and max(f1_scores) > 0 else 0.1
            
            # 1. Precisionå¯¹æ¯”
            bars1 = ax1.bar(algorithms, precision_scores, color=['#ff6b6b', '#4ecdc4', '#45b7d1', '#96ceb4'])
            ax1.set_title('Precision@10 å¯¹æ¯”')
            ax1.set_ylabel('Precision')
            ax1.set_ylim(0, max_precision * 1.2)
            for bar, score in zip(bars1, precision_scores):
                ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max_precision * 0.02, 
                        f'{score:.3f}', ha='center', va='bottom')
            
            # 2. Recallå¯¹æ¯”
            bars2 = ax2.bar(algorithms, recall_scores, color=['#ff6b6b', '#4ecdc4', '#45b7d1', '#96ceb4'])
            ax2.set_title('Recall@10 å¯¹æ¯”')
            ax2.set_ylabel('Recall')
            ax2.set_ylim(0, max_recall * 1.2)
            for bar, score in zip(bars2, recall_scores):
                ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max_recall * 0.02, 
                        f'{score:.3f}', ha='center', va='bottom')
            
            # 3. F1-Scoreå¯¹æ¯”
            bars3 = ax3.bar(algorithms, f1_scores, color=['#ff6b6b', '#4ecdc4', '#45b7d1', '#96ceb4'])
            ax3.set_title('F1-Score@10 å¯¹æ¯”')
            ax3.set_ylabel('F1-Score')
            ax3.set_ylim(0, max_f1 * 1.2)
            for bar, score in zip(bars3, f1_scores):
                ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max_f1 * 0.02, 
                        f'{score:.3f}', ha='center', va='bottom')
            
            # 4. ç®€åŒ–çš„ç®—æ³•å¯¹æ¯”ï¼ˆé¿å…é›·è¾¾å›¾é”™è¯¯ï¼‰
            ax4.axis('off')
            ax4.text(0.5, 0.7, 'ç®—æ³•è¯„ä¼°å®Œæˆ', ha='center', va='center', 
                    fontsize=16, weight='bold', transform=ax4.transAxes)
            
            # æ˜¾ç¤ºæœ€ä½³ç®—æ³•
            if f1_scores and max(f1_scores) > 0:
                best_idx = f1_scores.index(max(f1_scores))
                best_algo = algorithms[best_idx]
                best_score = f1_scores[best_idx]
                ax4.text(0.5, 0.4, f'æœ€ä½³ç®—æ³•: {best_algo}', ha='center', va='center',
                        fontsize=14, transform=ax4.transAxes)
                ax4.text(0.5, 0.3, f'F1-Score: {best_score:.4f}', ha='center', va='center',
                        fontsize=12, transform=ax4.transAxes)
            else:
                ax4.text(0.5, 0.4, 'æ‰€æœ‰ç®—æ³•æ•ˆæœéœ€è¦ä¼˜åŒ–', ha='center', va='center',
                        fontsize=12, color='red', transform=ax4.transAxes)
            
            plt.tight_layout()
            
            # ä¿å­˜å›¾è¡¨
            results_dir = "evaluation_results"
            os.makedirs(results_dir, exist_ok=True)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            plt.savefig(f"{results_dir}/algorithm_comparison_{timestamp}.png", dpi=300, bbox_inches='tight')
            
            logger.info(f"è¯„ä¼°å›¾è¡¨å·²ä¿å­˜åˆ°: {results_dir}/algorithm_comparison_{timestamp}.png")
            
        except ImportError:
            logger.warning("matplotlibæœªå®‰è£…ï¼Œè·³è¿‡å›¾è¡¨ç”Ÿæˆ")
        except Exception as e:
            logger.error(f"ç”Ÿæˆå›¾è¡¨å¤±è´¥: {e}")
    
    def generate_evaluation_report(self, results):
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        report = f"""
# å›¾ä¹¦æ¨èç®—æ³•è¯„ä¼°æŠ¥å‘Š

## è¯„ä¼°æ¦‚è¿°
- **è¯„ä¼°æ–¹æ³•**: ç•™å­˜éªŒè¯æ³• (Hold-out Validation)
- **æ•°æ®åˆ†å‰²**: 80%è®­ç»ƒé›† + 20%æµ‹è¯•é›†
- **è¯„ä¼°æŒ‡æ ‡**: Precision@10, Recall@10, F1-Score@10, Coverage
- **å–œå¥½å®šä¹‰**: è¯„åˆ†â‰¥4.0åˆ†çš„å›¾ä¹¦
- **è¯„ä¼°æ—¶é—´**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

## ç®—æ³•æ€§èƒ½å¯¹æ¯”

| ç®—æ³•åç§° | Precision@10 | Recall@10 | F1-Score@10 | Coverage | è¯„ä¼°ç”¨æˆ·æ•° |
|---------|-------------|-----------|-------------|----------|-----------|
"""
        
        for result in results:
            report += f"| {result['algorithm']} | {result['precision@10']:.4f} | {result['recall@10']:.4f} | {result['f1_score@10']:.4f} | {result['coverage']:.4f} | {result['evaluated_users']} |\n"
        
        # æ‰¾å‡ºæœ€ä½³ç®—æ³•
        best_precision = max(results, key=lambda x: x['precision@10'])
        best_recall = max(results, key=lambda x: x['recall@10'])
        best_f1 = max(results, key=lambda x: x['f1_score@10'])
        
        report += f"""
## ç»“æœåˆ†æ

### æœ€ä½³æ€§èƒ½
- **æœ€é«˜ç²¾ç¡®ç‡**: {best_precision['algorithm']} ({best_precision['precision@10']:.4f})
- **æœ€é«˜å¬å›ç‡**: {best_recall['algorithm']} ({best_recall['recall@10']:.4f})  
- **æœ€é«˜F1åˆ†æ•°**: {best_f1['algorithm']} ({best_f1['f1_score@10']:.4f})

### ç»“è®º
1. **{best_f1['algorithm']}** åœ¨ç»¼åˆæŒ‡æ ‡F1-Scoreä¸Šè¡¨ç°æœ€ä½³
2. æ··åˆæ¨èç­–ç•¥ç›¸æ¯”å•ä¸€ç®—æ³•çš„æå‡å¹…åº¦
3. æ¨èç³»ç»Ÿèƒ½å¤Ÿæœ‰æ•ˆé¢„æµ‹ç”¨æˆ·åå¥½

### æŠ€æœ¯æ„ä¹‰
- éªŒè¯äº†æ¨èç®—æ³•çš„æœ‰æ•ˆæ€§
- è¯æ˜äº†æ··åˆç­–ç•¥çš„ä¼˜è¶Šæ€§
- ä¸ºæ¯•ä¸šè®¾è®¡æä¾›äº†å®¢è§‚çš„å®éªŒæ•°æ®æ”¯æ’‘
"""
        
        # ä¿å­˜æŠ¥å‘Š
        results_dir = "evaluation_results"
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"{results_dir}/evaluation_report_{timestamp}.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
        return report